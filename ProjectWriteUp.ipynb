{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ** Build a Traffic Sign Recognition Project **\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "• Load the data set (see below for links to the project data set)\n",
    "• Explore, summarize and visualize the data set\n",
    "• Design, train and test a model architecture\n",
    "• Use the model to make predictions on new images\n",
    "• Analyze the softmax probabilities of the new images\n",
    "• Summarize the results with a written report\n",
    "• Rubric Points\n",
    "\n",
    "### README \n",
    "This is my project README. This file provides an overview of the project and details of each step taken to answer asked questions. The code part of project can be found at below link:\n",
    "\n",
    "\n",
    "### Data Set Summary & Exploration\n",
    "\n",
    "Provided data set for the project is already devided to three sets; training, validation and test set. Before trying out to build a model and make prediction, let's get more insight into the data; with measuring the length of each data set, I could see the size of each set and find out how many sample points are in each set. Below you can see each set's size;\n",
    "\n",
    "Number of training examples = 34799\n",
    "Number of testing examples = 12630\n",
    "Number of validation examples = 4410\n",
    "Image data shape = (32, 32, 3)\n",
    "Number of classes = 43\n",
    "\n",
    "### Include an exploratory visualization of the dataset.\n",
    "\n",
    "To further understand the data, I visulized few of the images and also explored on the distribution of different classes in each set. From the histograms it is clear that the data sets do not have equal samples of all classes and some of the classes have more occurances than the others. Though, between training, test and validation set, this distribution is more or less the same.\n",
    "\n",
    "### Design and Test a Model Architecture\n",
    "\n",
    "I used training set to train the model and then measure its performance on the validation set and tweaked the model until desired accuracy was achieved. \n",
    "\n",
    "The first thing I did was to normalize the data. In the previous lessons we learned that you can normalize the data to have a zero mean and reasonable varianve by x=(x-128)/128 but in this case it does not yield to the best result and validation accuracy; I changed the normalization to x=x/255 where 255 is the maximum value each pixel can have. I completely skipped over making 0 to be the mean; I believe the reason for lower performance with zero mean is due to use of ReLues as activation function; shifting the mean to zero means some of the pixel values will be negative; then after applying the weights and biases, the logits may still stay negative. Then, when activation function is applied, all the negative values turn to zero and do not activate the node. Therefore, some of the nodes do not contribute to the network and the performance stays low.\n",
    "\n",
    "I followed the same methodolgy used in class, using 2 convolution layers and 3 fully connected layers. I used pooling after each convolution layer. The only difference with what we used in class is the filter size and number of used filters; I used larger filters with higher depth in convolution layers which resulted in more nodes in the first fully connected layer.\n",
    "\n",
    "\n",
    "My final model consisted of the following layers:\n",
    "\n",
    "| Layer | Description |Output Size|\n",
    "| -------- | ---------- | -----|\n",
    "|first|convolution|26x26x8|\n",
    "||pooling|13x13x8|\n",
    "|second|convolution|8x8x22|\n",
    "||pooling|4x4x22|\n",
    "||flaten|352|\n",
    "|third|fully connected|120|\n",
    "|fourth|fully connected|84|\n",
    "|fifth|fully connected|43|\n",
    "\n",
    "Lastly, softmax was applied to get probabilities of belonging to each class.\n",
    "\n",
    "\n",
    "To train the model, I used Adam optimizer; in the first epoch, weights and biases were randomly assigned, then at the end of layers and after softmax, the loss (collective error of model) was calculated; then using the optimizer, derivative of error respect to each weight and bias was calculated to find out how much each one of them contributed to the error and how they should be modified. Then, weights and biases were updated and evertything repeated until good validation accuracy was obtained. \n",
    "\n",
    "\n",
    "My final model results were:\n",
    "\n",
    "training set accuracy of 99.9%\n",
    "\n",
    "validation set accuracy of 94%\n",
    "\n",
    "test set accuracy of 93.6%\n",
    "\n",
    "web test accuracy of 80%\n",
    "\n",
    "### Test a Model on New Images\n",
    "\n",
    "I chose randomly five pictures from German Traffic Data set and ran the trained model. The prediction for all 5 was correct. The results are summarized below\n",
    "\n",
    "Here are the results of the prediction:\n",
    "\n",
    "\n",
    "|Image |\tPrediction|\n",
    "|----|----|\n",
    "|No vehicles|No vehicles|\n",
    "|Speed limit (50km/h)|Speed limit (50km/h)|\n",
    "|Speed limit (80km/h)|Speed limit (120km/h)|\n",
    "|Speed limit (50km/h)|Speed limit (50km/h)|\n",
    "|Priority road|Priority road|\n",
    "\n",
    "Although the model performed well on 80% of these images, the test set accuracy is 93%! There are some images which include two signs and those are hard to predict. Also, there are a lot of similarities between images; for example many of them include a red circle around the image which makes a lot of pixels to look similar. This is especially pronounced when the image quality is low or the sign occupies a small portion of the image; I think the latest has been part of the reason in the wrong prediction in above sampled images from test set.\n",
    "\n",
    "On the images downloaded from web, the only wrong prediction comes from a sign which the model has not seen in the training set;\n",
    "\n",
    "|Image |\tPrediction|\n",
    "|----|----|\n",
    "|Traffic signals|Traffic signals|\n",
    "|Keep right|Keep right|\n",
    "|Priority road|Priority road|\n",
    "|Length limit (10 meters)|Go straight or left|\n",
    "|No entry|No entry|\n",
    "\n",
    "Regarding the sample set from test dataset, on the first two and the last two predictions the model is certain since the softmax probabilities are 100%. On the remaining image, the one which prediction is wrong, the model is only 53% certain this is the right class and the right answer is not among the top 5 probabilities."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
